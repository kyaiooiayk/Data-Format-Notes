{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** How to read csv files\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.090988Z",
     "start_time": "2023-07-04T12:54:16.914900Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.096887Z",
     "start_time": "2023-07-04T12:54:17.092629Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = np.random.random(size = (1000,20))\n",
    "d2 = np.random.random(size = (1000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.102274Z",
     "start_time": "2023-07-04T12:54:17.099216Z"
    }
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('./data/example.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.113155Z",
     "start_time": "2023-07-04T12:54:17.106286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"example.h5\" (mode r+)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.119360Z",
     "start_time": "2023-07-04T12:54:17.114971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_2\": shape (1000, 200), type \"<f8\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.create_dataset('dataset_1', data=d1)\n",
    "hf.create_dataset('dataset_2', data=d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.124146Z",
     "start_time": "2023-07-04T12:54:17.121051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"example.h5\" (mode r+)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.127842Z",
     "start_time": "2023-07-04T12:54:17.125380Z"
    }
   },
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.134101Z",
     "start_time": "2023-07-04T12:54:17.131582Z"
    }
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('./data/example.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.139208Z",
     "start_time": "2023-07-04T12:54:17.135442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['dataset_1', 'dataset_2']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.144081Z",
     "start_time": "2023-07-04T12:54:17.140712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_1\": shape (1000, 20), type \"<f8\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.get('dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.148251Z",
     "start_time": "2023-07-04T12:54:17.145890Z"
    }
   },
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating group\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- Groups are the basic container mechanism in a HDF5 file, allowing hierarchical organisation of the data.\n",
    "- Groups are created similarly to datasets, and datsets are then added using the group object.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.157010Z",
     "start_time": "2023-07-04T12:54:17.150351Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = np.random.random(size = (100,33))\n",
    "d2 = np.random.random(size = (100,333))\n",
    "d3 = np.random.random(size = (100,3333))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.162139Z",
     "start_time": "2023-07-04T12:54:17.158751Z"
    }
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('data.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.167546Z",
     "start_time": "2023-07-04T12:54:17.164748Z"
    }
   },
   "outputs": [],
   "source": [
    "g1 = hf.create_group('group1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.174465Z",
     "start_time": "2023-07-04T12:54:17.169691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data2\": shape (100, 33), type \"<f8\">"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.create_dataset('data1',data=d1)\n",
    "g1.create_dataset('data2',data=d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.178568Z",
     "start_time": "2023-07-04T12:54:17.176241Z"
    }
   },
   "outputs": [],
   "source": [
    "g2 = hf.create_group('group2/subfolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.185388Z",
     "start_time": "2023-07-04T12:54:17.180388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data3\": shape (100, 3333), type \"<f8\">"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2.create_dataset('data3',data=d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.189606Z",
     "start_time": "2023-07-04T12:54:17.187193Z"
    }
   },
   "outputs": [],
   "source": [
    "group2 = hf.get('group2/subfolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.195396Z",
     "start_time": "2023-07-04T12:54:17.191804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemsViewHDF5(<HDF5 group \"/group2/subfolder\" (1 members)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group2.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.199599Z",
     "start_time": "2023-07-04T12:54:17.197258Z"
    }
   },
   "outputs": [],
   "source": [
    "group1 = hf.get('group1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.204786Z",
     "start_time": "2023-07-04T12:54:17.201437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemsViewHDF5(<HDF5 group \"/group1\" (2 members)>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- To save on disk space, while sacrificing read speed, you can compress the data. Just add the compression argument, which can be either gzip, lzf or szip. gzip is the most portable, as itâ€™s available with every HDF5 install, lzf is the fastest but doesnâ€™t compress as effectively as gzip, and szip is a NASA format that is patented up; if you donâ€™t know about it, chances are your organisation doesnâ€™t have the patent, so avoid.\n",
    "- For gzip you can also specify the additional compression_opts argument, which sets the compression level. The default is 4, but it can be an integer between 0 and 9.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.219564Z",
     "start_time": "2023-07-04T12:54:17.206281Z"
    }
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('./data/example_compressed.h5', 'w')\n",
    "\n",
    "hf.create_dataset('dataset_1', data=d1, compression=\"gzip\", compression_opts=9)\n",
    "hf.create_dataset('dataset_2', data=d2, compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [h5py: reading and writing HDF5 files in Python](https://www.christopherlovell.co.uk/blog/2016/04/27/h5py-intro.html)\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:54:17.253838Z",
     "start_time": "2023-07-04T12:54:17.221105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.7\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "Compiler    : Clang 10.0.0 \n",
      "OS          : Darwin\n",
      "Release     : 22.5.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "h5py    : 3.6.0\n",
      "autopep8: 1.6.0\n",
      "json    : 2.0.9\n",
      "numpy   : 1.22.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -iv -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
